# ğŸŒ Universal NLP + Web Search + LLM Chatbot

A **Streamlit**â€‘based chat application that combines lightweight NLP, realâ€‘time internet search, and a local openâ€‘source LLM to deliver upâ€‘toâ€‘date answers in a friendly chat UI.

---

## ğŸ¯ Project Overview

This repository implements a modular â€œGenAI Labâ€ toolkit featuring:

1. **Natural Language Understanding**  
   â€“ Extracts keywords/entities from your question using **spaCy**.  
2. **Realâ€‘Time Web Search**  
   â€“ Queries Google via **SerpAPI** to fetch fresh snippets.  
3. **Local LLM Inference**  
   â€“ Runs a quantized **LLaMA** model (`llama-cpp-python`) onâ€‘device.  
4. **Chat UI & Memory**  
   â€“ Persistent chat history with Streamlitâ€™s builtâ€‘in `st.chat_message` & `st.chat_input`.  
5. **Caching & Performance**  
   â€“ Caches search results and model outputs to minimize waiting.  

Use this as your **freelance GenAI playground**, demoing everything from AI chatbots to analytics dashboards and more.

---

## ğŸš€ Features

- **Keyword / Intent Extraction**  
- **Dynamic Google Search** (via SerpAPI)  
- **Context Summarization** (simple truncation + optional pipeline)  
- **Local LLaMA Chat** (no external LLM calls)  
- **Conversation Memory** (inâ€‘session)  
- **Streamlit Chat UI** for interactive Q&A  
- **Result & Model Caching** for instant repeat queries  
- **Modular Structure**â€”easily swap in new data sources or LLMs  

---

## ğŸ“ Repository Structure

genai_lab/
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ serpapi.txt # Your SerpAPI key (do NOT commit)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ llama_model/
â”‚ â””â”€â”€ llama-2-7b.Q4_K_M.gguf
â”œâ”€â”€ modules/
â”‚ â”œâ”€â”€ nlp_utils.py
â”‚ â”œâ”€â”€ web_search.py
â”‚ â”œâ”€â”€ llm_utils.py
â”‚ |â”€â”€ summarizer.py
â”‚ â”œâ”€â”€ memory.py
â”‚ |â”€â”€ open_source_data.py
â”‚ â””â”€â”€ translator.py
â””â”€â”€ .github/
â””â”€â”€ workflows/
â””â”€â”€ ci.yml

---

## ğŸ› ï¸ Installation

1. **Clone** this repo:
   ```bash
   git clone https://github.com/yourusername/genai_lab.git
   cd genai_lab
Create & activate a Python virtual environment:

python -m venv venv
. venv/Scripts/activate    # Windows
# source venv/bin/activate # macOS/Linux
Install dependencies:

pip install -r requirements.txt
python -m spacy download en_core_web_sm
Add your SerpAPI key:

echo "YOUR_SERPAPI_KEY" > serpapi.txt
Download a quantized LLaMA model (Q4_K_M) to: llama_model/llama-2-7b.Q4_K_M.gguf

---


## â–¶ï¸ Usage

streamlit run app.py
Open your browser at http://localhost:8501

Chat naturally: the bot will extract keywords, search online, and answer via LLaMA.

---


## âš™ï¸ Configuration
Edit top of app.py to adjust:

Number of web snippets (num_results)

LLaMA model path & max_tokens

Cache TTLs in @st.cache_data decorators

---


## ğŸ§ª CI/CD Pipeline
We use GitHub Actions to:

-Run flake8 lint

-Install dependencies & spaCy model

-Smokeâ€‘test import of each module

-(Optional) Deploy to Streamlit Cloud

---


## ğŸš« Security & Best Practices
-Do not commit your serpapi.txt or any API keys.

-Add serpapi.txt and llama_model/ to .gitignore.

---


## ğŸ¤ Contributing
-Fork the repo

-Create a feature branch (git checkout -b feature/foo)

-Commit your changes & push

-Open a Pull Request

---


## ğŸ“„ License
-This project is licensed under the MIT License. See LICENSE for details.

---


### How to Enable CI/CD

1. Push this repo to GitHub in a publicâ€¯or private repo.  
2. Enable **Actions** tabâ€”youâ€™ll see the â€œCIâ€ workflow run on each push/PR.  
3. (Optional) Link your Streamlit Cloud account in the `deploy` step.

---

That should give you a **selfâ€‘documented repo** with **automated testing** and optional **deployment**. Let me know if youâ€™d like to customize further!







